{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import spacy\n",
    "import html\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import hstack\n",
    "from time import time\n",
    "from pprint import pprint\n",
    "from tabulate import tabulate\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.feature_extraction.text import CountVectorizer,\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTE: Make sure to remove non-printable ascii characters using:\n",
    "# tr -cd '\\11\\12\\15\\40-\\176' < file-with-binary-chars > clean-file\n",
    "\n",
    "def load_data(train_file, test_file):\n",
    "    train_data = [json.loads(line) for line in open(train_file).readlines()]\n",
    "    test_data = [json.loads(line) for line in open(test_file).readlines()]\n",
    "\n",
    "    train = {}\n",
    "    train['review'] = [html.unescape(sample['reviewText']) for sample in train_data]\n",
    "    train['summary'] = [html.unescape(sample['summary']) for sample in train_data]\n",
    "    train['rating'] = np.array([sample['overall'] for sample in train_data])\n",
    "\n",
    "    test = {}\n",
    "    test['review'] = [html.unescape(sample['reviewText']) for sample in test_data]\n",
    "    test['summary'] = [html.unescape(sample['summary']) for sample in test_data]\n",
    "    test['rating'] = np.array([sample['overall'] for sample in test_data])\n",
    "\n",
    "    classes = np.array([-1, 0, 1])\n",
    "\n",
    "    def target(rating):\n",
    "        if rating <= 2:\n",
    "            return classes[0]\n",
    "        elif rating == 3:\n",
    "            return classes[1]\n",
    "        else:\n",
    "            return classes[2]\n",
    "    train['target'] = np.array([target(rating) for rating in train['rating']])\n",
    "    test['target'] = np.array([target(rating) for rating in test['rating']])\n",
    "\n",
    "    return train, test, classes\n",
    "\n",
    "def load_preprocessed_data(datafile):\n",
    "    samples = [line.split('\\t') for line in open(datafile).readlines()]\n",
    "    samples = [sample for sample in samples if len(sample) == 3]\n",
    "    X = pd.DataFrame({\n",
    "        'review': [sample[0] for sample in samples],\n",
    "        'summary': [sample[1] for sample in samples]})\n",
    "    Y = [int(sample[2].strip()) for sample in samples]\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test, classes = load_data('audio_train.json', 'audio_dev.json')\n",
    "trainX, trainY = load_preprocessed_data('train_tok_clean.txt')\n",
    "testX, testY = load_preprocessed_data('test_tok_clean.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define vectorizers\n",
    "class SpacyCountVectorizer(CountVectorizer):\n",
    "    def __init__(self, lowercase=True, ngram_range=(1,1), binary=False, vocabulary=None,\n",
    "                 max_features=None, max_df=1.0, min_df=1, pos=True):\n",
    "        super(SpacyCountVectorizer, self).__init__(lowercase=lowercase, ngram_range=ngram_range, binary=binary, vocabulary=vocabulary,\n",
    "                                                   max_features=max_features, max_df=max_df, min_df=min_df)\n",
    "        self.pos = pos\n",
    "    def tokenize(self, doc):\n",
    "        if doc == '':\n",
    "            return []\n",
    "        if self.pos:\n",
    "            return doc.split('  ')\n",
    "        else:\n",
    "            return [tok.split(':|:')[0] for feat in features]\n",
    "    def build_tokenizer(self):\n",
    "        return lambda doc: self.tokenize(doc)\n",
    "\n",
    "class ReviewExtractor(object):\n",
    "    def transform(self, X):\n",
    "        return X['review']\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "class SummaryExtractor(object):\n",
    "    def transform(self, X):\n",
    "        return X['summ']\n",
    "    def fit(self, X, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_vectorizer = SpacyCountVectorizer(ngram_range=(1,2), binary=True, max_df=0.8, min_df=5e-6)\n",
    "train_rev_feat = rev_vectorizer.fit_transform(trainX['review'])\n",
    "test_rev_feat = rev_vectorizer.transform(testX['review'])\n",
    "\n",
    "summ_vectorizer = SpacyCountVectorizer(ngram_range=(1,2), binary=True, max_df=0.8, min_df=5e-6)\n",
    "train_summ_feat = summ_vectorizer.fit_transform(trainX['summary'])\n",
    "test_summ_feat = summ_vectorizer.transform(testX['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_feat1 = hstack([train_rev_feat, 3 * train_summ_feat])\n",
    "test_feat1 = hstack([test_rev_feat, 3 * test_summ_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set features\n",
    "train_feat = train_feat1; test_feat = test_feat1\n",
    "print(train_feat.shape)\n",
    "print(test_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_results(preds, dev_target):\n",
    "    n_correct = (preds == dev_target).sum()\n",
    "    print(\"accuracy={:4.2f} ({}/{})\".format(n_correct/preds.shape[0] * 100, n_correct, preds.shape[0]))\n",
    "    print(confusion_matrix(dev_target, preds, labels=[-1, 0, 1]))\n",
    "    print(\"macro-F1={:4.4f}\".format(f1_score(dev_target, preds, labels=[-1, 0, 1], average='macro')))\n",
    "    \n",
    "def evaluate_feat_MNB(train_feat, train_target, dev_feat, dev_target, class_prior=None):\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(train_feat, train_target)\n",
    "    preds = nb.predict(dev_feat)\n",
    "    display_results(preds, dev_target)\n",
    "    return nb\n",
    "\n",
    "def evaluate_feat_LR(train_feat, train_target, dev_feat, dev_target):\n",
    "    clf = LogisticRegression(penalty=\"l2\", C=1, solver='liblinear', multi_class='ovr', n_jobs=3, \n",
    "                             random_state=2324,max_iter=50, class_weight=\"balanced\")\n",
    "    clf.fit(train_feat, train_target)\n",
    "    preds = clf.predict(dev_feat)\n",
    "    display_results(preds, dev_target)\n",
    "    return clf\n",
    "\n",
    "def evaluate_feat_SGD(train_feat, train_target, dev_feat, dev_target, loss='hinge', \n",
    "                      penalty='l2', max_iter=None, average=True, alpha=0.0001, tol=1e-3):\n",
    "    clf = SGDClassifier(loss=loss, penalty=penalty, random_state=2324, max_iter=max_iter, \n",
    "                        class_weight=\"balanced\", average=average, alpha=alpha, tol=tol)\n",
    "    clf.fit(train_feat, train_target)\n",
    "    preds = clf.predict(dev_feat)\n",
    "    display_results(preds, dev_target)\n",
    "    return clf\n",
    "\n",
    "def evaluate_feat_SVM(train_feat, train_target, dev_feat, dev_target):\n",
    "    clf = LinearSVC(penalty=\"l2\", max_iter=50, random_state=2324, \n",
    "                    class_weight=\"balanced\", tol=1e-3)\n",
    "    clf.fit(train_feat, train_target)\n",
    "    preds = clf.predict(dev_feat)\n",
    "    display_results(preds, dev_target)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nb = evaluate_feat_MNB(train_feat, trainY, test_feat, testY)\n",
    "# lr = evaluate_feat_LR(train_feat, trainY, test_feat, testY)\n",
    "# lr = evaluate_feat_SGD(train_feat, trainY, test_feat, testY, loss='log')\n",
    "# svm2 = evaluate_feat_SVM(train_feat, trainY, test_feat, testY)\n",
    "svm1 = evaluate_feat_SGD(train_feat, trainY, test_feat, testY, loss='hinge',penalty='l2', average=True, alpha=1e-4, tol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(svm1, '/home/cse/dual/cs5130298/scratch/NLP/1/models/5.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
